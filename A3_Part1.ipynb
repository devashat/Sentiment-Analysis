{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    !pip install -q -U tensorflow-addons\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.test.is_gpu_available():\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"nlp\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ciFhUOmvCaW",
        "outputId": "7dca73b3-7b46-4e7a-f244-2cffea20384c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-9fd9f1f21091>:22: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n",
            "Go to Runtime > Change runtime and select a GPU hardware accelerator.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "VYvthkrXvFRW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Implementation\n",
        "\n",
        "Starter code taken from readings provided to us.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A3VS8ADRW-S4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HJ4GjKWwRtFI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from collections import Counter\n",
        "import tensorflow_datasets as tfds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
        "\n",
        "X_train[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULwxsSgSRuAu",
        "outputId": "6be7febd-36c8-4ed9-abfa-d8eabc2d873e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    id_to_word[id_] = token\n",
        "\n",
        "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_2Pq32A3SAJP",
        "outputId": "0f08beec-cd06-4fd3-dbbb-34009588c7c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<sos> this film was just brilliant casting location scenery story'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "train_size = info.splits[\"train\"].num_examples\n",
        "test_size = info.splits[\"test\"].num_examples"
      ],
      "metadata": {
        "id": "663hvCjgVPUR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
        "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f38MmvVbiVZs",
        "outputId": "d0131836-227d-4283-b4da-914807689cdf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
            "Label: 0 = Negative\n",
            "\n",
            "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
            "Label: 0 = Negative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X_batch, y_batch):\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
        "\n",
        "preprocess(X_batch, y_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aniqa_NkVXn8",
        "outputId": "e65d8d1b-5947-4f2b-8f0e-e6aee234fbde"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n",
              " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
              "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
              "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
              "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
              "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
              "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
              "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
              "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
              "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
              "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
              "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
              "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
              "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
              "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
              "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
              "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
              "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
              "         b'Cons']], dtype=object)>,\n",
              " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        vocabulary.update(list(review.numpy()))\n",
        "\n",
        "vocabulary.most_common()[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXYK7s0kVdI-",
        "outputId": "6908d9e9-08fb-45b2-caab-a9190f1933f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7kDzRrXwEgc",
        "outputId": "73d0efcd-7b0d-4335-f107-674e4edb54b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53893"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "truncated_vocabulary = [\n",
        "word for word, count in vocabulary.most_common()[:vocab_size]]"
      ],
      "metadata": {
        "id": "ziDgwlZdVkEo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
        "for word in b\"This movie was faaaaaantastic\".split():\n",
        "    print(word_to_id.get(word) or vocab_size)"
      ],
      "metadata": {
        "id": "dQl75V1ywReM",
        "outputId": "be8e356c-b8cb-4c22-dda1-20eccffcb2af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n",
            "12\n",
            "11\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
        "\n",
        "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWNYcxNjV2AD",
        "outputId": "50b98932-93b0-4758-ba22-df6b49434be0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_words(X_batch, y_batch):\n",
        "    return table.lookup(X_batch), y_batch\n",
        "\n",
        "\n",
        "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
        "test_set = datasets[\"test\"].batch(32).map(preprocess)\n",
        "\n",
        "train_set = train_set.map(encode_words).cache().prefetch(1)\n",
        "test_set = test_set.map(encode_words).cache().prefetch(1)"
      ],
      "metadata": {
        "id": "EYcX0zbZV6c0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 16\n",
        "hidden_dim = 64\n",
        "dropout_rate = 0.5\n",
        "nonlinearity = 'tanh'\n",
        "learning_rate = 1e-3\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_dim,\n",
        "                           mask_zero=True, input_shape=[None]),\n",
        "    keras.layers.SimpleRNN(hidden_dim, activation=nonlinearity),\n",
        "    keras.layers.Dropout(dropout_rate),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQGncXcLWPVI",
        "outputId": "814f8b56-0437-464f-a7a2-e25daeec33ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 29s 34ms/step - loss: 0.6864 - accuracy: 0.5371\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 31s 40ms/step - loss: 0.5187 - accuracy: 0.7552\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.3759 - accuracy: 0.8434\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.2422 - accuracy: 0.9060\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.1664 - accuracy: 0.9385\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 0.1184 - accuracy: 0.9574\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.0931 - accuracy: 0.9681\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.0547 - accuracy: 0.9817\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 26s 34ms/step - loss: 0.0586 - accuracy: 0.9802\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.0367 - accuracy: 0.9874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train set\\n\",model.evaluate(train_set, steps=train_size // 32, batch_size=32))\n",
        "\n",
        "print(\"Test set\\n\",model.evaluate(test_set, steps=test_size // 32, batch_size=32))"
      ],
      "metadata": {
        "id": "C35QzLIPWlW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d834cc-041a-4346-d027-fb27fbca7ef8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "781/781 [==============================] - 9s 10ms/step - loss: 0.0385 - accuracy: 0.9860\n",
            "Train set\n",
            " [0.038450371474027634, 0.9859955310821533]\n",
            "781/781 [==============================] - 10s 12ms/step - loss: 1.7259 - accuracy: 0.6605\n",
            "Test set\n",
            " [1.72585928440094, 0.6604913473129272]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Implementation\n",
        "\n",
        "Starter code taken from readings provided to us\n",
        "\n",
        "Since we preprocess the data and create the final training set above, all that is left to do is to simply create the LSTM model. For this, a simple substitution in our earlier code is enough."
      ],
      "metadata": {
        "id": "qJ8GRhKhfInZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_dim, mask_zero=True, input_shape=[None]),\n",
        "    keras.layers.LSTM(hidden_dim, activation=nonlinearity),\n",
        "    keras.layers.Dropout(dropout_rate),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, epochs=epochs)"
      ],
      "metadata": {
        "id": "Q0iV_hZDgDKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6f5d3e-4f49-4be1-d521-94130ae554f8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 53s 58ms/step - loss: 0.5408 - accuracy: 0.7157\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.3827 - accuracy: 0.8358\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.3224 - accuracy: 0.8714\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.2732 - accuracy: 0.8969\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 45s 58ms/step - loss: 0.2268 - accuracy: 0.9164\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 46s 59ms/step - loss: 0.1914 - accuracy: 0.9329\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.1658 - accuracy: 0.9428\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.1405 - accuracy: 0.9526\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 46s 58ms/step - loss: 0.1267 - accuracy: 0.9556\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1039 - accuracy: 0.9651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train set\\n\",model.evaluate(train_set, steps=train_size // 32, batch_size=32))\n",
        "\n",
        "print(\"Test set\\n\",model.evaluate(test_set, steps=test_size // 32, batch_size=32))"
      ],
      "metadata": {
        "id": "EQThIyjagGPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d00ce07-d492-4462-e3d8-0541d1c34c96"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "781/781 [==============================] - 14s 15ms/step - loss: 0.1675 - accuracy: 0.9417\n",
            "Train set\n",
            " [0.16747130453586578, 0.9416613578796387]\n",
            "781/781 [==============================] - 15s 19ms/step - loss: 1.1215 - accuracy: 0.7221\n",
            "Test set\n",
            " [1.1215111017227173, 0.7220710515975952]\n"
          ]
        }
      ]
    }
  ]
}